{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ffe028b9",
   "metadata": {},
   "source": [
    "# 1. Transfer learning: Shallow learning vs Finetuning, Pytorch cont"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba93861a",
   "metadata": {},
   "source": [
    "## 1.2. Feature Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7c295a0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encode output into dummy variables\n",
    "import torch\n",
    "from torch import nn\n",
    "from torchvision.models import resnet50\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "# preprocess images via PyTorch docs\n",
    "from torchvision import transforms\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "# utils\n",
    "import os\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "from json import load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4a745011",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_raw_data(ann):\n",
    "    X_raw = []\n",
    "    y = []\n",
    "    for img_data, ann_data in zip(ann['images'], ann['annotations']):\n",
    "        assert ann_data['id'] == img_data['id']\n",
    "        y.append(ann_data['category_id'])\n",
    "        img = Image.open('decathlon-1.0/' + img_data['file_name'])\n",
    "        X_raw.append(np.array(img))\n",
    "        img.close()\n",
    "    return X_raw, y\n",
    "\n",
    "def preprocess(X_raw, transformer_fn):\n",
    "    X = []\n",
    "    for img in X_raw:\n",
    "        img_pil = Image.fromarray(img)\n",
    "        proc_img = transformer_fn(img_pil)\n",
    "        X.append(proc_img)\n",
    "    return torch.stack(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0b08313b",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('decathlon-1.0/annotations/vgg-flowers_train.json') as f:\n",
    "    ann_train = load(f)\n",
    "    \n",
    "with open('decathlon-1.0/annotations/vgg-flowers_val.json') as f:\n",
    "    ann_val = load(f)\n",
    "\n",
    "X_train_raw, y_train = get_raw_data(ann_train)\n",
    "X_val_raw, y_val = get_raw_data(ann_val)\n",
    "\n",
    "\n",
    "dummifyer = OneHotEncoder(sparse = False)\n",
    "dummifyer.fit(np.array(y_train).reshape(-1, 1))\n",
    "\n",
    "y_train_dummy = torch.from_numpy(dummifyer.transform(np.array(y_train).reshape(-1, 1)))\n",
    "y_val_dummy = torch.from_numpy(dummifyer.transform(np.array(y_val).reshape(-1, 1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ea41a9d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# hyperparams\n",
    "\n",
    "# set the same value of hyperparameters (learning rate=0.001, momentum=0.9) for all the layers\n",
    "momentum = 0.9\n",
    "batch_size = 64\n",
    "\n",
    "loss_fn = torch.nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d0240e99",
   "metadata": {},
   "outputs": [],
   "source": [
    "# preprocess images\n",
    "transformer_fn = transforms.Compose([\n",
    "    transforms.Resize(256),\n",
    "    transforms.CenterCrop(224),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "])\n",
    "\n",
    "X_train = preprocess(X_train_raw, transformer_fn)\n",
    "X_val = preprocess(X_val_raw, transformer_fn)\n",
    "\n",
    "# make data loader\n",
    "train_loader = DataLoader(TensorDataset(X_train, y_train_dummy), batch_size = batch_size, shuffle = True)\n",
    "val_loader = DataLoader(TensorDataset(X_val, y_val_dummy), batch_size = batch_size, shuffle = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2a389624",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n"
     ]
    }
   ],
   "source": [
    "# move data to GPU\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)\n",
    "\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2d1f588b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# helper function for generating a fresh ResNet\n",
    "def get_resnet_feature_extractor(num_classes):\n",
    "    resnet = resnet50(pretrained = True)\n",
    "    \n",
    "    # freeze layers\n",
    "    for params in resnet.parameters():\n",
    "        params.requires_grad = False\n",
    "    \n",
    "    # change the final fully connected layer output to the number of classes in the target dataset.\n",
    "    resnet.fc = nn.Linear(2048, num_classes)\n",
    "    resnet.fc.requires_grad = True # unfreeze top layer\n",
    "    \n",
    "    return resnet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "135c9c46",
   "metadata": {},
   "outputs": [],
   "source": [
    "lrs = [1, 0.1, 0.01, 0.001]\n",
    "#lrs = [1]\n",
    "\n",
    "nets = [get_resnet_feature_extractor(102) for i in range(len(lrs))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f80cef9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train loop\n",
    "def train(model, tloader, vloader, lf, optim, epochs = 10):\n",
    "    \n",
    "    train_loss_history = []\n",
    "    val_loss_history = []\n",
    "    \n",
    "    for t in range(epochs):\n",
    "\n",
    "        epoch_train_loss = 0\n",
    "        epoch_val_loss = 0\n",
    "        model.train()\n",
    "        for i, (X_local, y_local) in enumerate(tloader):\n",
    "            \n",
    "            X_local = X_local.to(device)\n",
    "            y_local = y_local.to(device)\n",
    "\n",
    "            # Forward pass: compute predicted y by passing x to the model.\n",
    "            y_pred_local = model(X_local)\n",
    "\n",
    "            # batch loss\n",
    "            loss = lf(y_pred_local, y_local)\n",
    "            epoch_train_loss += loss.item()\n",
    "            #print(\"batch: %d/%d\\tbatch loss: %.2f\" % (i + 1, n_batches, loss.item()))\n",
    "\n",
    "            # compute gradient\n",
    "            optim.zero_grad()\n",
    "            loss.backward()\n",
    "\n",
    "            # update params\n",
    "            optim.step()\n",
    "\n",
    "        # get validation loss\n",
    "        model.eval()\n",
    "        for i, (X_local, y_local) in enumerate(vloader):\n",
    "            \n",
    "            X_local = X_local.to(device)\n",
    "            y_local = y_local.to(device)\n",
    "\n",
    "            # disable autograd\n",
    "            with torch.no_grad():\n",
    "                y_pred_local = model(X_local)\n",
    "\n",
    "                # batch loss\n",
    "                loss = lf(y_pred_local, y_local)\n",
    "                epoch_val_loss += loss.item()\n",
    "\n",
    "        # show epoch validation loss\n",
    "        train_loss_history.append(epoch_train_loss)\n",
    "        val_loss_history.append(epoch_val_loss)\n",
    "\n",
    "        print(\"\\t\\t\\t\\tEpoch: %d/%d\\tTrain loss: %.2f\\tValid loss: %.2f\" \n",
    "              % (t + 1, epochs, epoch_train_loss, epoch_val_loss))\n",
    "    \n",
    "    return train_loss_history, val_loss_history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d3a20bbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy(model, loader, encoder):\n",
    "\n",
    "    correct = 0\n",
    "    n = 0\n",
    "    \n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "    for X_local, y_local in loader:\n",
    "        \n",
    "        X_local = X_local.to(device)\n",
    "        y_local = y_local.to(device)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            y_logits = model(X_local)\n",
    "\n",
    "            y_pred = torch.argmax(y_logits, axis = 1)\n",
    "            y_true = torch.argmax(y_local, axis = 1)\n",
    "    \n",
    "        correct += sum(y_pred == y_true).item()\n",
    "        n += len(y_true)\n",
    "    \n",
    "    model.to('cpu')\n",
    "    return correct / n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8e5894c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "========================================\n",
      "learning rate 1.00e+00\n",
      "========================================\n",
      "\t\t\t\tEpoch: 1/200\tTrain loss: 1778.09\tValid loss: 4419.54\n",
      "\t\t\t\tEpoch: 2/200\tTrain loss: 5540.22\tValid loss: 5221.01\n",
      "\t\t\t\tEpoch: 3/200\tTrain loss: 4157.00\tValid loss: 3441.71\n",
      "\t\t\t\tEpoch: 4/200\tTrain loss: 2011.60\tValid loss: 1691.89\n",
      "\t\t\t\tEpoch: 5/200\tTrain loss: 924.23\tValid loss: 1050.89\n",
      "\t\t\t\tEpoch: 6/200\tTrain loss: 414.79\tValid loss: 611.66\n",
      "\t\t\t\tEpoch: 7/200\tTrain loss: 118.17\tValid loss: 481.75\n",
      "\t\t\t\tEpoch: 8/200\tTrain loss: 48.77\tValid loss: 412.24\n",
      "\t\t\t\tEpoch: 9/200\tTrain loss: 27.03\tValid loss: 396.76\n",
      "\t\t\t\tEpoch: 10/200\tTrain loss: 17.64\tValid loss: 338.52\n",
      "\t\t\t\tEpoch: 11/200\tTrain loss: 14.19\tValid loss: 342.59\n",
      "\t\t\t\tEpoch: 12/200\tTrain loss: 12.55\tValid loss: 344.38\n",
      "\t\t\t\tEpoch: 13/200\tTrain loss: 12.93\tValid loss: 315.61\n",
      "\t\t\t\tEpoch: 14/200\tTrain loss: 5.96\tValid loss: 306.62\n",
      "\t\t\t\tEpoch: 15/200\tTrain loss: 3.59\tValid loss: 348.02\n",
      "\t\t\t\tEpoch: 16/200\tTrain loss: 2.22\tValid loss: 349.06\n",
      "\t\t\t\tEpoch: 17/200\tTrain loss: 2.50\tValid loss: 314.52\n",
      "\t\t\t\tEpoch: 18/200\tTrain loss: 1.93\tValid loss: 330.31\n",
      "\t\t\t\tEpoch: 19/200\tTrain loss: 1.54\tValid loss: 310.76\n",
      "\t\t\t\tEpoch: 20/200\tTrain loss: 0.93\tValid loss: 302.36\n",
      "\t\t\t\tEpoch: 21/200\tTrain loss: 3.45\tValid loss: 316.08\n",
      "\t\t\t\tEpoch: 22/200\tTrain loss: 2.46\tValid loss: 310.36\n",
      "\t\t\t\tEpoch: 23/200\tTrain loss: 1.84\tValid loss: 290.94\n",
      "\t\t\t\tEpoch: 24/200\tTrain loss: 1.46\tValid loss: 319.10\n",
      "\t\t\t\tEpoch: 25/200\tTrain loss: 2.02\tValid loss: 363.91\n",
      "\t\t\t\tEpoch: 26/200\tTrain loss: 1.35\tValid loss: 307.33\n",
      "\t\t\t\tEpoch: 27/200\tTrain loss: 0.76\tValid loss: 298.00\n",
      "\t\t\t\tEpoch: 28/200\tTrain loss: 0.60\tValid loss: 298.97\n",
      "\t\t\t\tEpoch: 29/200\tTrain loss: 0.93\tValid loss: 306.13\n",
      "\t\t\t\tEpoch: 30/200\tTrain loss: 0.30\tValid loss: 276.25\n",
      "\t\t\t\tEpoch: 31/200\tTrain loss: 0.17\tValid loss: 281.62\n",
      "\t\t\t\tEpoch: 32/200\tTrain loss: 0.23\tValid loss: 278.27\n",
      "\t\t\t\tEpoch: 33/200\tTrain loss: 0.22\tValid loss: 269.75\n",
      "\t\t\t\tEpoch: 34/200\tTrain loss: 0.02\tValid loss: 274.36\n",
      "\t\t\t\tEpoch: 35/200\tTrain loss: 0.45\tValid loss: 278.46\n",
      "\t\t\t\tEpoch: 36/200\tTrain loss: 0.47\tValid loss: 299.15\n",
      "\t\t\t\tEpoch: 37/200\tTrain loss: 1.35\tValid loss: 322.99\n",
      "\t\t\t\tEpoch: 38/200\tTrain loss: 2.49\tValid loss: 326.95\n",
      "\t\t\t\tEpoch: 39/200\tTrain loss: 8.15\tValid loss: 310.67\n",
      "\t\t\t\tEpoch: 40/200\tTrain loss: 5.44\tValid loss: 327.52\n",
      "\t\t\t\tEpoch: 41/200\tTrain loss: 2.15\tValid loss: 314.12\n",
      "\t\t\t\tEpoch: 42/200\tTrain loss: 0.90\tValid loss: 287.04\n",
      "\t\t\t\tEpoch: 43/200\tTrain loss: 0.23\tValid loss: 315.84\n",
      "\t\t\t\tEpoch: 44/200\tTrain loss: 0.78\tValid loss: 317.24\n",
      "\t\t\t\tEpoch: 45/200\tTrain loss: 0.29\tValid loss: 303.52\n",
      "\t\t\t\tEpoch: 46/200\tTrain loss: 2.08\tValid loss: 298.90\n",
      "\t\t\t\tEpoch: 47/200\tTrain loss: 1.27\tValid loss: 289.97\n",
      "\t\t\t\tEpoch: 48/200\tTrain loss: 1.54\tValid loss: 296.27\n",
      "\t\t\t\tEpoch: 49/200\tTrain loss: 0.75\tValid loss: 304.09\n",
      "\t\t\t\tEpoch: 50/200\tTrain loss: 0.89\tValid loss: 309.94\n",
      "\t\t\t\tEpoch: 51/200\tTrain loss: 1.17\tValid loss: 327.24\n",
      "\t\t\t\tEpoch: 52/200\tTrain loss: 0.83\tValid loss: 326.04\n",
      "\t\t\t\tEpoch: 53/200\tTrain loss: 0.10\tValid loss: 315.96\n",
      "\t\t\t\tEpoch: 54/200\tTrain loss: 0.00\tValid loss: 314.52\n",
      "\t\t\t\tEpoch: 55/200\tTrain loss: 0.20\tValid loss: 299.30\n",
      "\t\t\t\tEpoch: 56/200\tTrain loss: 1.01\tValid loss: 297.95\n",
      "\t\t\t\tEpoch: 57/200\tTrain loss: 1.25\tValid loss: 297.04\n",
      "\t\t\t\tEpoch: 58/200\tTrain loss: 0.16\tValid loss: 307.55\n",
      "\t\t\t\tEpoch: 59/200\tTrain loss: 0.95\tValid loss: 298.26\n",
      "\t\t\t\tEpoch: 60/200\tTrain loss: 0.02\tValid loss: 298.15\n",
      "\t\t\t\tEpoch: 61/200\tTrain loss: 0.00\tValid loss: 303.01\n",
      "\t\t\t\tEpoch: 62/200\tTrain loss: 0.00\tValid loss: 302.36\n",
      "\t\t\t\tEpoch: 63/200\tTrain loss: 0.00\tValid loss: 299.15\n",
      "\t\t\t\tEpoch: 64/200\tTrain loss: 0.43\tValid loss: 299.05\n",
      "\t\t\t\tEpoch: 65/200\tTrain loss: 0.48\tValid loss: 298.05\n",
      "\t\t\t\tEpoch: 66/200\tTrain loss: 0.09\tValid loss: 294.05\n",
      "\t\t\t\tEpoch: 67/200\tTrain loss: 0.08\tValid loss: 298.93\n",
      "\t\t\t\tEpoch: 68/200\tTrain loss: 0.15\tValid loss: 293.82\n",
      "\t\t\t\tEpoch: 69/200\tTrain loss: 0.00\tValid loss: 300.67\n",
      "\t\t\t\tEpoch: 70/200\tTrain loss: 0.23\tValid loss: 284.90\n",
      "\t\t\t\tEpoch: 71/200\tTrain loss: 0.00\tValid loss: 288.34\n",
      "\t\t\t\tEpoch: 72/200\tTrain loss: 0.01\tValid loss: 292.50\n",
      "\t\t\t\tEpoch: 73/200\tTrain loss: 0.00\tValid loss: 294.57\n",
      "\t\t\t\tEpoch: 74/200\tTrain loss: 0.00\tValid loss: 295.95\n",
      "\t\t\t\tEpoch: 75/200\tTrain loss: 0.00\tValid loss: 295.44\n",
      "\t\t\t\tEpoch: 76/200\tTrain loss: 0.00\tValid loss: 292.49\n",
      "\t\t\t\tEpoch: 77/200\tTrain loss: 0.11\tValid loss: 288.59\n",
      "\t\t\t\tEpoch: 78/200\tTrain loss: 0.08\tValid loss: 296.40\n",
      "\t\t\t\tEpoch: 79/200\tTrain loss: 0.00\tValid loss: 290.02\n",
      "\t\t\t\tEpoch: 80/200\tTrain loss: 0.00\tValid loss: 292.94\n",
      "\t\t\t\tEpoch: 81/200\tTrain loss: 0.00\tValid loss: 293.55\n",
      "\t\t\t\tEpoch: 82/200\tTrain loss: 0.00\tValid loss: 292.86\n",
      "\t\t\t\tEpoch: 83/200\tTrain loss: 0.09\tValid loss: 289.51\n",
      "\t\t\t\tEpoch: 84/200\tTrain loss: 0.00\tValid loss: 298.04\n",
      "\t\t\t\tEpoch: 85/200\tTrain loss: 0.01\tValid loss: 298.39\n",
      "\t\t\t\tEpoch: 86/200\tTrain loss: 0.00\tValid loss: 299.75\n",
      "\t\t\t\tEpoch: 87/200\tTrain loss: 0.17\tValid loss: 294.67\n",
      "\t\t\t\tEpoch: 88/200\tTrain loss: 0.07\tValid loss: 295.74\n",
      "\t\t\t\tEpoch: 89/200\tTrain loss: 0.00\tValid loss: 297.72\n",
      "\t\t\t\tEpoch: 90/200\tTrain loss: 0.00\tValid loss: 298.26\n",
      "\t\t\t\tEpoch: 91/200\tTrain loss: 0.00\tValid loss: 299.71\n",
      "\t\t\t\tEpoch: 92/200\tTrain loss: 0.00\tValid loss: 295.16\n",
      "\t\t\t\tEpoch: 93/200\tTrain loss: 0.06\tValid loss: 289.22\n",
      "\t\t\t\tEpoch: 94/200\tTrain loss: 0.00\tValid loss: 292.66\n",
      "\t\t\t\tEpoch: 95/200\tTrain loss: 0.00\tValid loss: 292.02\n",
      "\t\t\t\tEpoch: 96/200\tTrain loss: 0.00\tValid loss: 290.24\n",
      "\t\t\t\tEpoch: 97/200\tTrain loss: 0.00\tValid loss: 289.55\n",
      "\t\t\t\tEpoch: 98/200\tTrain loss: 0.00\tValid loss: 289.74\n",
      "\t\t\t\tEpoch: 99/200\tTrain loss: 0.00\tValid loss: 290.76\n",
      "\t\t\t\tEpoch: 100/200\tTrain loss: 0.00\tValid loss: 291.06\n",
      "\t\t\t\tEpoch: 101/200\tTrain loss: 0.00\tValid loss: 289.46\n",
      "\t\t\t\tEpoch: 102/200\tTrain loss: 0.00\tValid loss: 288.55\n",
      "\t\t\t\tEpoch: 103/200\tTrain loss: 0.00\tValid loss: 290.42\n",
      "\t\t\t\tEpoch: 104/200\tTrain loss: 0.00\tValid loss: 290.39\n",
      "\t\t\t\tEpoch: 105/200\tTrain loss: 0.00\tValid loss: 290.12\n",
      "\t\t\t\tEpoch: 106/200\tTrain loss: 0.05\tValid loss: 291.11\n",
      "\t\t\t\tEpoch: 107/200\tTrain loss: 0.18\tValid loss: 298.46\n",
      "\t\t\t\tEpoch: 108/200\tTrain loss: 0.03\tValid loss: 299.42\n",
      "\t\t\t\tEpoch: 109/200\tTrain loss: 0.03\tValid loss: 293.88\n",
      "\t\t\t\tEpoch: 110/200\tTrain loss: 0.58\tValid loss: 288.77\n",
      "\t\t\t\tEpoch: 111/200\tTrain loss: 0.00\tValid loss: 296.95\n",
      "\t\t\t\tEpoch: 112/200\tTrain loss: 0.38\tValid loss: 280.06\n",
      "\t\t\t\tEpoch: 113/200\tTrain loss: 0.00\tValid loss: 294.03\n",
      "\t\t\t\tEpoch: 114/200\tTrain loss: 0.08\tValid loss: 294.75\n",
      "\t\t\t\tEpoch: 115/200\tTrain loss: 0.06\tValid loss: 285.75\n",
      "\t\t\t\tEpoch: 116/200\tTrain loss: 0.12\tValid loss: 288.05\n",
      "\t\t\t\tEpoch: 117/200\tTrain loss: 0.71\tValid loss: 280.45\n",
      "\t\t\t\tEpoch: 118/200\tTrain loss: 0.00\tValid loss: 290.31\n",
      "\t\t\t\tEpoch: 119/200\tTrain loss: 0.25\tValid loss: 282.16\n",
      "\t\t\t\tEpoch: 120/200\tTrain loss: 0.00\tValid loss: 285.91\n",
      "\t\t\t\tEpoch: 121/200\tTrain loss: 0.28\tValid loss: 303.23\n",
      "\t\t\t\tEpoch: 122/200\tTrain loss: 0.64\tValid loss: 291.20\n",
      "\t\t\t\tEpoch: 123/200\tTrain loss: 0.06\tValid loss: 291.39\n",
      "\t\t\t\tEpoch: 124/200\tTrain loss: 0.06\tValid loss: 284.02\n",
      "\t\t\t\tEpoch: 125/200\tTrain loss: 0.00\tValid loss: 279.71\n",
      "\t\t\t\tEpoch: 126/200\tTrain loss: 0.16\tValid loss: 274.07\n",
      "\t\t\t\tEpoch: 127/200\tTrain loss: 0.34\tValid loss: 283.57\n",
      "\t\t\t\tEpoch: 128/200\tTrain loss: 0.27\tValid loss: 286.69\n",
      "\t\t\t\tEpoch: 129/200\tTrain loss: 0.00\tValid loss: 290.72\n",
      "\t\t\t\tEpoch: 130/200\tTrain loss: 0.20\tValid loss: 287.54\n",
      "\t\t\t\tEpoch: 131/200\tTrain loss: 0.00\tValid loss: 301.24\n",
      "\t\t\t\tEpoch: 132/200\tTrain loss: 0.25\tValid loss: 282.74\n",
      "\t\t\t\tEpoch: 133/200\tTrain loss: 0.00\tValid loss: 282.82\n",
      "\t\t\t\tEpoch: 134/200\tTrain loss: 0.00\tValid loss: 280.91\n",
      "\t\t\t\tEpoch: 135/200\tTrain loss: 0.00\tValid loss: 282.41\n",
      "\t\t\t\tEpoch: 136/200\tTrain loss: 0.00\tValid loss: 280.94\n",
      "\t\t\t\tEpoch: 137/200\tTrain loss: 0.00\tValid loss: 280.91\n",
      "\t\t\t\tEpoch: 138/200\tTrain loss: 0.00\tValid loss: 283.97\n",
      "\t\t\t\tEpoch: 139/200\tTrain loss: 0.00\tValid loss: 279.95\n",
      "\t\t\t\tEpoch: 140/200\tTrain loss: 0.00\tValid loss: 281.54\n",
      "\t\t\t\tEpoch: 141/200\tTrain loss: 0.00\tValid loss: 282.57\n",
      "\t\t\t\tEpoch: 142/200\tTrain loss: 0.00\tValid loss: 281.52\n",
      "\t\t\t\tEpoch: 143/200\tTrain loss: 0.00\tValid loss: 281.65\n",
      "\t\t\t\tEpoch: 144/200\tTrain loss: 0.00\tValid loss: 280.55\n",
      "\t\t\t\tEpoch: 145/200\tTrain loss: 0.00\tValid loss: 281.96\n",
      "\t\t\t\tEpoch: 146/200\tTrain loss: 0.00\tValid loss: 280.16\n",
      "\t\t\t\tEpoch: 147/200\tTrain loss: 0.00\tValid loss: 281.20\n",
      "\t\t\t\tEpoch: 148/200\tTrain loss: 0.00\tValid loss: 281.76\n",
      "\t\t\t\tEpoch: 149/200\tTrain loss: 0.00\tValid loss: 281.88\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t\t\t\tEpoch: 150/200\tTrain loss: 0.00\tValid loss: 280.70\n",
      "\t\t\t\tEpoch: 151/200\tTrain loss: 0.00\tValid loss: 280.69\n",
      "\t\t\t\tEpoch: 152/200\tTrain loss: 0.00\tValid loss: 280.45\n",
      "\t\t\t\tEpoch: 153/200\tTrain loss: 0.00\tValid loss: 278.98\n",
      "\t\t\t\tEpoch: 154/200\tTrain loss: 0.00\tValid loss: 278.71\n",
      "\t\t\t\tEpoch: 155/200\tTrain loss: 0.00\tValid loss: 280.30\n",
      "\t\t\t\tEpoch: 156/200\tTrain loss: 0.00\tValid loss: 280.37\n",
      "\t\t\t\tEpoch: 157/200\tTrain loss: 0.00\tValid loss: 280.25\n",
      "\t\t\t\tEpoch: 158/200\tTrain loss: 0.00\tValid loss: 277.81\n",
      "\t\t\t\tEpoch: 159/200\tTrain loss: 0.00\tValid loss: 280.11\n",
      "\t\t\t\tEpoch: 160/200\tTrain loss: 0.08\tValid loss: 283.18\n",
      "\t\t\t\tEpoch: 161/200\tTrain loss: 0.20\tValid loss: 273.69\n",
      "\t\t\t\tEpoch: 162/200\tTrain loss: 0.00\tValid loss: 277.42\n",
      "\t\t\t\tEpoch: 163/200\tTrain loss: 0.00\tValid loss: 275.87\n",
      "\t\t\t\tEpoch: 164/200\tTrain loss: 0.00\tValid loss: 277.80\n",
      "\t\t\t\tEpoch: 165/200\tTrain loss: 0.00\tValid loss: 278.80\n",
      "\t\t\t\tEpoch: 166/200\tTrain loss: 0.00\tValid loss: 277.17\n",
      "\t\t\t\tEpoch: 167/200\tTrain loss: 0.00\tValid loss: 279.73\n",
      "\t\t\t\tEpoch: 168/200\tTrain loss: 0.00\tValid loss: 277.49\n",
      "\t\t\t\tEpoch: 169/200\tTrain loss: 0.00\tValid loss: 277.89\n",
      "\t\t\t\tEpoch: 170/200\tTrain loss: 0.00\tValid loss: 278.77\n",
      "\t\t\t\tEpoch: 171/200\tTrain loss: 0.00\tValid loss: 276.80\n",
      "\t\t\t\tEpoch: 172/200\tTrain loss: 0.00\tValid loss: 275.53\n",
      "\t\t\t\tEpoch: 173/200\tTrain loss: 0.00\tValid loss: 277.69\n",
      "\t\t\t\tEpoch: 174/200\tTrain loss: 0.00\tValid loss: 276.89\n",
      "\t\t\t\tEpoch: 175/200\tTrain loss: 0.04\tValid loss: 270.97\n",
      "\t\t\t\tEpoch: 176/200\tTrain loss: 0.00\tValid loss: 275.05\n",
      "\t\t\t\tEpoch: 177/200\tTrain loss: 0.00\tValid loss: 276.16\n",
      "\t\t\t\tEpoch: 178/200\tTrain loss: 0.00\tValid loss: 275.40\n",
      "\t\t\t\tEpoch: 179/200\tTrain loss: 0.00\tValid loss: 275.48\n",
      "\t\t\t\tEpoch: 180/200\tTrain loss: 0.00\tValid loss: 275.24\n",
      "\t\t\t\tEpoch: 181/200\tTrain loss: 0.00\tValid loss: 274.99\n",
      "\t\t\t\tEpoch: 182/200\tTrain loss: 0.00\tValid loss: 275.63\n",
      "\t\t\t\tEpoch: 183/200\tTrain loss: 0.00\tValid loss: 275.48\n",
      "\t\t\t\tEpoch: 184/200\tTrain loss: 0.00\tValid loss: 276.13\n",
      "\t\t\t\tEpoch: 185/200\tTrain loss: 0.00\tValid loss: 272.98\n",
      "\t\t\t\tEpoch: 186/200\tTrain loss: 0.00\tValid loss: 272.65\n",
      "\t\t\t\tEpoch: 187/200\tTrain loss: 0.00\tValid loss: 274.50\n",
      "\t\t\t\tEpoch: 188/200\tTrain loss: 0.00\tValid loss: 272.46\n",
      "\t\t\t\tEpoch: 189/200\tTrain loss: 0.00\tValid loss: 271.83\n",
      "\t\t\t\tEpoch: 190/200\tTrain loss: 0.00\tValid loss: 273.77\n",
      "\t\t\t\tEpoch: 191/200\tTrain loss: 0.00\tValid loss: 272.75\n",
      "\t\t\t\tEpoch: 192/200\tTrain loss: 0.00\tValid loss: 273.12\n",
      "\t\t\t\tEpoch: 193/200\tTrain loss: 0.00\tValid loss: 271.25\n",
      "\t\t\t\tEpoch: 194/200\tTrain loss: 0.00\tValid loss: 272.47\n",
      "\t\t\t\tEpoch: 195/200\tTrain loss: 0.00\tValid loss: 272.22\n",
      "\t\t\t\tEpoch: 196/200\tTrain loss: 0.00\tValid loss: 273.26\n",
      "\t\t\t\tEpoch: 197/200\tTrain loss: 0.00\tValid loss: 271.79\n",
      "\t\t\t\tEpoch: 198/200\tTrain loss: 0.00\tValid loss: 274.54\n",
      "\t\t\t\tEpoch: 199/200\tTrain loss: 0.00\tValid loss: 272.98\n",
      "\t\t\t\tEpoch: 200/200\tTrain loss: 0.00\tValid loss: 271.68\n",
      "final train loss: 0.000\tfinal val loss: 271.681\n",
      "\n",
      "\n",
      "========================================\n",
      "learning rate 1.00e-01\n",
      "========================================\n",
      "\t\t\t\tEpoch: 1/200\tTrain loss: 79.18\tValid loss: 67.48\n",
      "\t\t\t\tEpoch: 2/200\tTrain loss: 48.08\tValid loss: 55.09\n",
      "\t\t\t\tEpoch: 3/200\tTrain loss: 20.30\tValid loss: 38.75\n",
      "\t\t\t\tEpoch: 4/200\tTrain loss: 10.57\tValid loss: 36.23\n",
      "\t\t\t\tEpoch: 5/200\tTrain loss: 5.34\tValid loss: 35.39\n",
      "\t\t\t\tEpoch: 6/200\tTrain loss: 2.84\tValid loss: 27.91\n",
      "\t\t\t\tEpoch: 7/200\tTrain loss: 1.33\tValid loss: 24.94\n",
      "\t\t\t\tEpoch: 8/200\tTrain loss: 0.58\tValid loss: 23.63\n",
      "\t\t\t\tEpoch: 9/200\tTrain loss: 0.33\tValid loss: 22.03\n",
      "\t\t\t\tEpoch: 10/200\tTrain loss: 0.25\tValid loss: 21.54\n",
      "\t\t\t\tEpoch: 11/200\tTrain loss: 0.19\tValid loss: 21.37\n",
      "\t\t\t\tEpoch: 12/200\tTrain loss: 0.19\tValid loss: 21.31\n",
      "\t\t\t\tEpoch: 13/200\tTrain loss: 0.21\tValid loss: 21.35\n",
      "\t\t\t\tEpoch: 14/200\tTrain loss: 0.17\tValid loss: 21.56\n",
      "\t\t\t\tEpoch: 15/200\tTrain loss: 0.18\tValid loss: 21.47\n",
      "\t\t\t\tEpoch: 16/200\tTrain loss: 0.14\tValid loss: 21.32\n",
      "\t\t\t\tEpoch: 17/200\tTrain loss: 0.14\tValid loss: 21.31\n",
      "\t\t\t\tEpoch: 18/200\tTrain loss: 0.14\tValid loss: 21.31\n",
      "\t\t\t\tEpoch: 19/200\tTrain loss: 0.13\tValid loss: 21.24\n",
      "\t\t\t\tEpoch: 20/200\tTrain loss: 0.12\tValid loss: 21.21\n",
      "\t\t\t\tEpoch: 21/200\tTrain loss: 0.13\tValid loss: 21.36\n",
      "\t\t\t\tEpoch: 22/200\tTrain loss: 0.12\tValid loss: 21.32\n",
      "\t\t\t\tEpoch: 23/200\tTrain loss: 0.12\tValid loss: 21.35\n",
      "\t\t\t\tEpoch: 24/200\tTrain loss: 0.12\tValid loss: 21.28\n",
      "\t\t\t\tEpoch: 25/200\tTrain loss: 0.11\tValid loss: 21.22\n",
      "\t\t\t\tEpoch: 26/200\tTrain loss: 0.11\tValid loss: 21.22\n",
      "\t\t\t\tEpoch: 27/200\tTrain loss: 0.11\tValid loss: 21.17\n",
      "\t\t\t\tEpoch: 28/200\tTrain loss: 0.11\tValid loss: 21.26\n",
      "\t\t\t\tEpoch: 29/200\tTrain loss: 0.11\tValid loss: 21.20\n",
      "\t\t\t\tEpoch: 30/200\tTrain loss: 0.10\tValid loss: 21.15\n",
      "\t\t\t\tEpoch: 31/200\tTrain loss: 0.10\tValid loss: 21.26\n",
      "\t\t\t\tEpoch: 32/200\tTrain loss: 0.09\tValid loss: 21.32\n",
      "\t\t\t\tEpoch: 33/200\tTrain loss: 0.09\tValid loss: 21.24\n",
      "\t\t\t\tEpoch: 34/200\tTrain loss: 0.08\tValid loss: 21.24\n",
      "\t\t\t\tEpoch: 35/200\tTrain loss: 0.08\tValid loss: 21.18\n",
      "\t\t\t\tEpoch: 36/200\tTrain loss: 0.09\tValid loss: 21.25\n",
      "\t\t\t\tEpoch: 37/200\tTrain loss: 0.09\tValid loss: 21.25\n",
      "\t\t\t\tEpoch: 38/200\tTrain loss: 0.08\tValid loss: 21.25\n",
      "\t\t\t\tEpoch: 39/200\tTrain loss: 0.09\tValid loss: 21.13\n",
      "\t\t\t\tEpoch: 40/200\tTrain loss: 0.09\tValid loss: 21.18\n",
      "\t\t\t\tEpoch: 41/200\tTrain loss: 0.09\tValid loss: 21.19\n",
      "\t\t\t\tEpoch: 42/200\tTrain loss: 0.09\tValid loss: 21.30\n",
      "\t\t\t\tEpoch: 43/200\tTrain loss: 0.08\tValid loss: 21.27\n",
      "\t\t\t\tEpoch: 44/200\tTrain loss: 0.08\tValid loss: 21.34\n",
      "\t\t\t\tEpoch: 45/200\tTrain loss: 0.08\tValid loss: 21.23\n",
      "\t\t\t\tEpoch: 46/200\tTrain loss: 0.08\tValid loss: 21.32\n",
      "\t\t\t\tEpoch: 47/200\tTrain loss: 0.07\tValid loss: 21.33\n",
      "\t\t\t\tEpoch: 48/200\tTrain loss: 0.07\tValid loss: 21.27\n",
      "\t\t\t\tEpoch: 49/200\tTrain loss: 0.07\tValid loss: 21.26\n",
      "\t\t\t\tEpoch: 50/200\tTrain loss: 0.07\tValid loss: 21.37\n",
      "\t\t\t\tEpoch: 51/200\tTrain loss: 0.08\tValid loss: 21.33\n",
      "\t\t\t\tEpoch: 52/200\tTrain loss: 0.07\tValid loss: 21.38\n",
      "\t\t\t\tEpoch: 53/200\tTrain loss: 0.07\tValid loss: 21.37\n",
      "\t\t\t\tEpoch: 54/200\tTrain loss: 0.08\tValid loss: 21.32\n",
      "\t\t\t\tEpoch: 55/200\tTrain loss: 0.07\tValid loss: 21.28\n",
      "\t\t\t\tEpoch: 56/200\tTrain loss: 0.07\tValid loss: 21.23\n",
      "\t\t\t\tEpoch: 57/200\tTrain loss: 0.07\tValid loss: 21.23\n",
      "\t\t\t\tEpoch: 58/200\tTrain loss: 0.07\tValid loss: 21.40\n",
      "\t\t\t\tEpoch: 59/200\tTrain loss: 0.07\tValid loss: 21.33\n",
      "\t\t\t\tEpoch: 60/200\tTrain loss: 0.07\tValid loss: 21.25\n",
      "\t\t\t\tEpoch: 61/200\tTrain loss: 0.06\tValid loss: 21.29\n",
      "\t\t\t\tEpoch: 62/200\tTrain loss: 0.06\tValid loss: 21.43\n",
      "\t\t\t\tEpoch: 63/200\tTrain loss: 0.06\tValid loss: 21.37\n",
      "\t\t\t\tEpoch: 64/200\tTrain loss: 0.06\tValid loss: 21.34\n",
      "\t\t\t\tEpoch: 65/200\tTrain loss: 0.06\tValid loss: 21.24\n",
      "\t\t\t\tEpoch: 66/200\tTrain loss: 0.06\tValid loss: 21.20\n",
      "\t\t\t\tEpoch: 67/200\tTrain loss: 0.06\tValid loss: 21.24\n",
      "\t\t\t\tEpoch: 68/200\tTrain loss: 0.06\tValid loss: 21.26\n",
      "\t\t\t\tEpoch: 69/200\tTrain loss: 0.06\tValid loss: 21.36\n",
      "\t\t\t\tEpoch: 70/200\tTrain loss: 0.06\tValid loss: 21.24\n",
      "\t\t\t\tEpoch: 71/200\tTrain loss: 0.06\tValid loss: 21.32\n",
      "\t\t\t\tEpoch: 72/200\tTrain loss: 0.06\tValid loss: 21.28\n",
      "\t\t\t\tEpoch: 73/200\tTrain loss: 0.05\tValid loss: 21.28\n",
      "\t\t\t\tEpoch: 74/200\tTrain loss: 0.06\tValid loss: 21.27\n",
      "\t\t\t\tEpoch: 75/200\tTrain loss: 0.06\tValid loss: 21.29\n",
      "\t\t\t\tEpoch: 76/200\tTrain loss: 0.05\tValid loss: 21.29\n",
      "\t\t\t\tEpoch: 77/200\tTrain loss: 0.05\tValid loss: 21.33\n",
      "\t\t\t\tEpoch: 78/200\tTrain loss: 0.05\tValid loss: 21.27\n",
      "\t\t\t\tEpoch: 79/200\tTrain loss: 0.05\tValid loss: 21.28\n",
      "\t\t\t\tEpoch: 80/200\tTrain loss: 0.05\tValid loss: 21.35\n",
      "\t\t\t\tEpoch: 81/200\tTrain loss: 0.05\tValid loss: 21.30\n",
      "\t\t\t\tEpoch: 82/200\tTrain loss: 0.06\tValid loss: 21.35\n",
      "\t\t\t\tEpoch: 83/200\tTrain loss: 0.05\tValid loss: 21.39\n",
      "\t\t\t\tEpoch: 84/200\tTrain loss: 0.05\tValid loss: 21.28\n",
      "\t\t\t\tEpoch: 85/200\tTrain loss: 0.05\tValid loss: 21.31\n",
      "\t\t\t\tEpoch: 86/200\tTrain loss: 0.05\tValid loss: 21.34\n",
      "\t\t\t\tEpoch: 87/200\tTrain loss: 0.05\tValid loss: 21.37\n",
      "\t\t\t\tEpoch: 88/200\tTrain loss: 0.05\tValid loss: 21.43\n",
      "\t\t\t\tEpoch: 89/200\tTrain loss: 0.05\tValid loss: 21.43\n",
      "\t\t\t\tEpoch: 90/200\tTrain loss: 0.05\tValid loss: 21.38\n",
      "\t\t\t\tEpoch: 91/200\tTrain loss: 0.05\tValid loss: 21.42\n",
      "\t\t\t\tEpoch: 92/200\tTrain loss: 0.05\tValid loss: 21.44\n",
      "\t\t\t\tEpoch: 93/200\tTrain loss: 0.05\tValid loss: 21.35\n",
      "\t\t\t\tEpoch: 94/200\tTrain loss: 0.05\tValid loss: 21.42\n",
      "\t\t\t\tEpoch: 95/200\tTrain loss: 0.05\tValid loss: 21.36\n",
      "\t\t\t\tEpoch: 96/200\tTrain loss: 0.04\tValid loss: 21.36\n",
      "\t\t\t\tEpoch: 97/200\tTrain loss: 0.05\tValid loss: 21.44\n",
      "\t\t\t\tEpoch: 98/200\tTrain loss: 0.05\tValid loss: 21.45\n",
      "\t\t\t\tEpoch: 99/200\tTrain loss: 0.04\tValid loss: 21.47\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t\t\t\tEpoch: 100/200\tTrain loss: 0.05\tValid loss: 21.41\n",
      "\t\t\t\tEpoch: 101/200\tTrain loss: 0.05\tValid loss: 21.41\n",
      "\t\t\t\tEpoch: 102/200\tTrain loss: 0.04\tValid loss: 21.46\n",
      "\t\t\t\tEpoch: 103/200\tTrain loss: 0.04\tValid loss: 21.44\n",
      "\t\t\t\tEpoch: 104/200\tTrain loss: 0.04\tValid loss: 21.45\n",
      "\t\t\t\tEpoch: 105/200\tTrain loss: 0.04\tValid loss: 21.41\n",
      "\t\t\t\tEpoch: 106/200\tTrain loss: 0.04\tValid loss: 21.39\n",
      "\t\t\t\tEpoch: 107/200\tTrain loss: 0.04\tValid loss: 21.44\n",
      "\t\t\t\tEpoch: 108/200\tTrain loss: 0.04\tValid loss: 21.36\n",
      "\t\t\t\tEpoch: 109/200\tTrain loss: 0.04\tValid loss: 21.43\n",
      "\t\t\t\tEpoch: 110/200\tTrain loss: 0.04\tValid loss: 21.43\n",
      "\t\t\t\tEpoch: 111/200\tTrain loss: 0.04\tValid loss: 21.53\n",
      "\t\t\t\tEpoch: 112/200\tTrain loss: 0.04\tValid loss: 21.56\n",
      "\t\t\t\tEpoch: 113/200\tTrain loss: 0.04\tValid loss: 21.43\n",
      "\t\t\t\tEpoch: 114/200\tTrain loss: 0.04\tValid loss: 21.46\n",
      "\t\t\t\tEpoch: 115/200\tTrain loss: 0.04\tValid loss: 21.42\n",
      "\t\t\t\tEpoch: 116/200\tTrain loss: 0.04\tValid loss: 21.53\n",
      "\t\t\t\tEpoch: 117/200\tTrain loss: 0.04\tValid loss: 21.44\n",
      "\t\t\t\tEpoch: 118/200\tTrain loss: 0.04\tValid loss: 21.42\n",
      "\t\t\t\tEpoch: 119/200\tTrain loss: 0.04\tValid loss: 21.47\n",
      "\t\t\t\tEpoch: 120/200\tTrain loss: 0.04\tValid loss: 21.51\n",
      "\t\t\t\tEpoch: 121/200\tTrain loss: 0.04\tValid loss: 21.49\n",
      "\t\t\t\tEpoch: 122/200\tTrain loss: 0.04\tValid loss: 21.44\n",
      "\t\t\t\tEpoch: 123/200\tTrain loss: 0.04\tValid loss: 21.49\n",
      "\t\t\t\tEpoch: 124/200\tTrain loss: 0.04\tValid loss: 21.48\n",
      "\t\t\t\tEpoch: 125/200\tTrain loss: 0.04\tValid loss: 21.48\n",
      "\t\t\t\tEpoch: 126/200\tTrain loss: 0.04\tValid loss: 21.48\n",
      "\t\t\t\tEpoch: 127/200\tTrain loss: 0.04\tValid loss: 21.54\n",
      "\t\t\t\tEpoch: 128/200\tTrain loss: 0.04\tValid loss: 21.49\n",
      "\t\t\t\tEpoch: 129/200\tTrain loss: 0.04\tValid loss: 21.48\n",
      "\t\t\t\tEpoch: 130/200\tTrain loss: 0.04\tValid loss: 21.48\n",
      "\t\t\t\tEpoch: 131/200\tTrain loss: 0.04\tValid loss: 21.43\n",
      "\t\t\t\tEpoch: 132/200\tTrain loss: 0.04\tValid loss: 21.40\n",
      "\t\t\t\tEpoch: 133/200\tTrain loss: 0.04\tValid loss: 21.52\n",
      "\t\t\t\tEpoch: 134/200\tTrain loss: 0.04\tValid loss: 21.46\n",
      "\t\t\t\tEpoch: 135/200\tTrain loss: 0.04\tValid loss: 21.61\n",
      "\t\t\t\tEpoch: 136/200\tTrain loss: 0.04\tValid loss: 21.58\n",
      "\t\t\t\tEpoch: 137/200\tTrain loss: 0.04\tValid loss: 21.56\n",
      "\t\t\t\tEpoch: 138/200\tTrain loss: 0.04\tValid loss: 21.51\n",
      "\t\t\t\tEpoch: 139/200\tTrain loss: 0.04\tValid loss: 21.56\n",
      "\t\t\t\tEpoch: 140/200\tTrain loss: 0.04\tValid loss: 21.50\n",
      "\t\t\t\tEpoch: 141/200\tTrain loss: 0.03\tValid loss: 21.48\n",
      "\t\t\t\tEpoch: 142/200\tTrain loss: 0.03\tValid loss: 21.45\n",
      "\t\t\t\tEpoch: 143/200\tTrain loss: 0.04\tValid loss: 21.54\n",
      "\t\t\t\tEpoch: 144/200\tTrain loss: 0.04\tValid loss: 21.44\n",
      "\t\t\t\tEpoch: 145/200\tTrain loss: 0.03\tValid loss: 21.46\n",
      "\t\t\t\tEpoch: 146/200\tTrain loss: 0.04\tValid loss: 21.48\n",
      "\t\t\t\tEpoch: 147/200\tTrain loss: 0.03\tValid loss: 21.42\n",
      "\t\t\t\tEpoch: 148/200\tTrain loss: 0.04\tValid loss: 21.43\n",
      "\t\t\t\tEpoch: 149/200\tTrain loss: 0.03\tValid loss: 21.51\n",
      "\t\t\t\tEpoch: 150/200\tTrain loss: 0.03\tValid loss: 21.60\n",
      "\t\t\t\tEpoch: 151/200\tTrain loss: 0.03\tValid loss: 21.49\n",
      "\t\t\t\tEpoch: 152/200\tTrain loss: 0.03\tValid loss: 21.52\n",
      "\t\t\t\tEpoch: 153/200\tTrain loss: 0.03\tValid loss: 21.55\n",
      "\t\t\t\tEpoch: 154/200\tTrain loss: 0.04\tValid loss: 21.62\n",
      "\t\t\t\tEpoch: 155/200\tTrain loss: 0.03\tValid loss: 21.62\n",
      "\t\t\t\tEpoch: 156/200\tTrain loss: 0.03\tValid loss: 21.61\n",
      "\t\t\t\tEpoch: 157/200\tTrain loss: 0.03\tValid loss: 21.57\n",
      "\t\t\t\tEpoch: 158/200\tTrain loss: 0.03\tValid loss: 21.52\n",
      "\t\t\t\tEpoch: 159/200\tTrain loss: 0.03\tValid loss: 21.59\n",
      "\t\t\t\tEpoch: 160/200\tTrain loss: 0.03\tValid loss: 21.55\n",
      "\t\t\t\tEpoch: 161/200\tTrain loss: 0.03\tValid loss: 21.59\n",
      "\t\t\t\tEpoch: 162/200\tTrain loss: 0.03\tValid loss: 21.57\n",
      "\t\t\t\tEpoch: 163/200\tTrain loss: 0.03\tValid loss: 21.62\n",
      "\t\t\t\tEpoch: 164/200\tTrain loss: 0.03\tValid loss: 21.60\n",
      "\t\t\t\tEpoch: 165/200\tTrain loss: 0.03\tValid loss: 21.53\n",
      "\t\t\t\tEpoch: 166/200\tTrain loss: 0.03\tValid loss: 21.63\n",
      "\t\t\t\tEpoch: 167/200\tTrain loss: 0.03\tValid loss: 21.67\n",
      "\t\t\t\tEpoch: 168/200\tTrain loss: 0.03\tValid loss: 21.56\n",
      "\t\t\t\tEpoch: 169/200\tTrain loss: 0.03\tValid loss: 21.59\n",
      "\t\t\t\tEpoch: 170/200\tTrain loss: 0.03\tValid loss: 21.59\n",
      "\t\t\t\tEpoch: 171/200\tTrain loss: 0.03\tValid loss: 21.63\n",
      "\t\t\t\tEpoch: 172/200\tTrain loss: 0.03\tValid loss: 21.72\n",
      "\t\t\t\tEpoch: 173/200\tTrain loss: 0.03\tValid loss: 21.66\n",
      "\t\t\t\tEpoch: 174/200\tTrain loss: 0.03\tValid loss: 21.58\n",
      "\t\t\t\tEpoch: 175/200\tTrain loss: 0.03\tValid loss: 21.59\n",
      "\t\t\t\tEpoch: 176/200\tTrain loss: 0.03\tValid loss: 21.64\n",
      "\t\t\t\tEpoch: 177/200\tTrain loss: 0.03\tValid loss: 21.62\n",
      "\t\t\t\tEpoch: 178/200\tTrain loss: 0.03\tValid loss: 21.68\n",
      "\t\t\t\tEpoch: 179/200\tTrain loss: 0.03\tValid loss: 21.67\n",
      "\t\t\t\tEpoch: 180/200\tTrain loss: 0.03\tValid loss: 21.58\n",
      "\t\t\t\tEpoch: 181/200\tTrain loss: 0.03\tValid loss: 21.62\n",
      "\t\t\t\tEpoch: 182/200\tTrain loss: 0.03\tValid loss: 21.65\n",
      "\t\t\t\tEpoch: 183/200\tTrain loss: 0.03\tValid loss: 21.67\n",
      "\t\t\t\tEpoch: 184/200\tTrain loss: 0.03\tValid loss: 21.58\n",
      "\t\t\t\tEpoch: 185/200\tTrain loss: 0.03\tValid loss: 21.58\n",
      "\t\t\t\tEpoch: 186/200\tTrain loss: 0.03\tValid loss: 21.58\n",
      "\t\t\t\tEpoch: 187/200\tTrain loss: 0.03\tValid loss: 21.59\n",
      "\t\t\t\tEpoch: 188/200\tTrain loss: 0.03\tValid loss: 21.67\n",
      "\t\t\t\tEpoch: 189/200\tTrain loss: 0.03\tValid loss: 21.68\n",
      "\t\t\t\tEpoch: 190/200\tTrain loss: 0.03\tValid loss: 21.72\n",
      "\t\t\t\tEpoch: 191/200\tTrain loss: 0.03\tValid loss: 21.68\n",
      "\t\t\t\tEpoch: 192/200\tTrain loss: 0.03\tValid loss: 21.69\n",
      "\t\t\t\tEpoch: 193/200\tTrain loss: 0.03\tValid loss: 21.67\n",
      "\t\t\t\tEpoch: 194/200\tTrain loss: 0.03\tValid loss: 21.69\n",
      "\t\t\t\tEpoch: 195/200\tTrain loss: 0.03\tValid loss: 21.58\n",
      "\t\t\t\tEpoch: 196/200\tTrain loss: 0.03\tValid loss: 21.69\n",
      "\t\t\t\tEpoch: 197/200\tTrain loss: 0.03\tValid loss: 21.67\n",
      "\t\t\t\tEpoch: 198/200\tTrain loss: 0.03\tValid loss: 21.65\n",
      "\t\t\t\tEpoch: 199/200\tTrain loss: 0.03\tValid loss: 21.66\n",
      "\t\t\t\tEpoch: 200/200\tTrain loss: 0.03\tValid loss: 21.74\n",
      "final train loss: 0.027\tfinal val loss: 21.741\n",
      "\n",
      "\n",
      "========================================\n",
      "learning rate 1.00e-02\n",
      "========================================\n",
      "\t\t\t\tEpoch: 1/200\tTrain loss: 74.23\tValid loss: 68.81\n",
      "\t\t\t\tEpoch: 2/200\tTrain loss: 64.06\tValid loss: 59.17\n",
      "\t\t\t\tEpoch: 3/200\tTrain loss: 52.32\tValid loss: 51.31\n",
      "\t\t\t\tEpoch: 4/200\tTrain loss: 42.32\tValid loss: 44.85\n",
      "\t\t\t\tEpoch: 5/200\tTrain loss: 34.40\tValid loss: 40.31\n",
      "\t\t\t\tEpoch: 6/200\tTrain loss: 28.35\tValid loss: 36.77\n",
      "\t\t\t\tEpoch: 7/200\tTrain loss: 23.65\tValid loss: 33.91\n",
      "\t\t\t\tEpoch: 8/200\tTrain loss: 20.13\tValid loss: 32.10\n",
      "\t\t\t\tEpoch: 9/200\tTrain loss: 17.04\tValid loss: 30.52\n",
      "\t\t\t\tEpoch: 10/200\tTrain loss: 14.94\tValid loss: 29.25\n",
      "\t\t\t\tEpoch: 11/200\tTrain loss: 13.37\tValid loss: 28.28\n",
      "\t\t\t\tEpoch: 12/200\tTrain loss: 11.93\tValid loss: 27.47\n",
      "\t\t\t\tEpoch: 13/200\tTrain loss: 10.47\tValid loss: 26.86\n",
      "\t\t\t\tEpoch: 14/200\tTrain loss: 9.43\tValid loss: 26.12\n",
      "\t\t\t\tEpoch: 15/200\tTrain loss: 8.63\tValid loss: 25.77\n",
      "\t\t\t\tEpoch: 16/200\tTrain loss: 7.81\tValid loss: 25.22\n",
      "\t\t\t\tEpoch: 17/200\tTrain loss: 7.17\tValid loss: 24.93\n",
      "\t\t\t\tEpoch: 18/200\tTrain loss: 6.59\tValid loss: 24.56\n",
      "\t\t\t\tEpoch: 19/200\tTrain loss: 6.00\tValid loss: 24.28\n",
      "\t\t\t\tEpoch: 20/200\tTrain loss: 5.56\tValid loss: 24.07\n",
      "\t\t\t\tEpoch: 21/200\tTrain loss: 5.22\tValid loss: 23.81\n",
      "\t\t\t\tEpoch: 22/200\tTrain loss: 4.87\tValid loss: 23.60\n",
      "\t\t\t\tEpoch: 23/200\tTrain loss: 4.68\tValid loss: 23.44\n",
      "\t\t\t\tEpoch: 24/200\tTrain loss: 4.39\tValid loss: 23.29\n",
      "\t\t\t\tEpoch: 25/200\tTrain loss: 4.07\tValid loss: 23.06\n",
      "\t\t\t\tEpoch: 26/200\tTrain loss: 3.84\tValid loss: 22.98\n",
      "\t\t\t\tEpoch: 27/200\tTrain loss: 3.61\tValid loss: 22.85\n",
      "\t\t\t\tEpoch: 28/200\tTrain loss: 3.45\tValid loss: 22.66\n",
      "\t\t\t\tEpoch: 29/200\tTrain loss: 3.40\tValid loss: 22.55\n",
      "\t\t\t\tEpoch: 30/200\tTrain loss: 3.20\tValid loss: 22.60\n",
      "\t\t\t\tEpoch: 31/200\tTrain loss: 2.97\tValid loss: 22.41\n",
      "\t\t\t\tEpoch: 32/200\tTrain loss: 3.02\tValid loss: 22.33\n",
      "\t\t\t\tEpoch: 33/200\tTrain loss: 2.65\tValid loss: 22.17\n",
      "\t\t\t\tEpoch: 34/200\tTrain loss: 2.59\tValid loss: 22.18\n",
      "\t\t\t\tEpoch: 35/200\tTrain loss: 2.62\tValid loss: 22.02\n",
      "\t\t\t\tEpoch: 36/200\tTrain loss: 2.37\tValid loss: 22.00\n",
      "\t\t\t\tEpoch: 37/200\tTrain loss: 2.43\tValid loss: 22.00\n",
      "\t\t\t\tEpoch: 38/200\tTrain loss: 2.28\tValid loss: 21.92\n",
      "\t\t\t\tEpoch: 39/200\tTrain loss: 2.15\tValid loss: 21.80\n",
      "\t\t\t\tEpoch: 40/200\tTrain loss: 2.14\tValid loss: 21.83\n",
      "\t\t\t\tEpoch: 41/200\tTrain loss: 2.05\tValid loss: 21.70\n",
      "\t\t\t\tEpoch: 42/200\tTrain loss: 1.98\tValid loss: 21.66\n",
      "\t\t\t\tEpoch: 43/200\tTrain loss: 1.95\tValid loss: 21.62\n",
      "\t\t\t\tEpoch: 44/200\tTrain loss: 1.86\tValid loss: 21.53\n",
      "\t\t\t\tEpoch: 45/200\tTrain loss: 1.82\tValid loss: 21.53\n",
      "\t\t\t\tEpoch: 46/200\tTrain loss: 1.73\tValid loss: 21.47\n",
      "\t\t\t\tEpoch: 47/200\tTrain loss: 1.76\tValid loss: 21.46\n",
      "\t\t\t\tEpoch: 48/200\tTrain loss: 1.72\tValid loss: 21.40\n",
      "\t\t\t\tEpoch: 49/200\tTrain loss: 1.65\tValid loss: 21.31\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t\t\t\tEpoch: 50/200\tTrain loss: 1.54\tValid loss: 21.35\n",
      "\t\t\t\tEpoch: 51/200\tTrain loss: 1.56\tValid loss: 21.32\n",
      "\t\t\t\tEpoch: 52/200\tTrain loss: 1.53\tValid loss: 21.22\n",
      "\t\t\t\tEpoch: 53/200\tTrain loss: 1.48\tValid loss: 21.23\n",
      "\t\t\t\tEpoch: 54/200\tTrain loss: 1.42\tValid loss: 21.17\n",
      "\t\t\t\tEpoch: 55/200\tTrain loss: 1.36\tValid loss: 21.13\n",
      "\t\t\t\tEpoch: 56/200\tTrain loss: 1.36\tValid loss: 21.20\n",
      "\t\t\t\tEpoch: 57/200\tTrain loss: 1.34\tValid loss: 21.13\n",
      "\t\t\t\tEpoch: 58/200\tTrain loss: 1.34\tValid loss: 21.12\n",
      "\t\t\t\tEpoch: 59/200\tTrain loss: 1.28\tValid loss: 21.08\n",
      "\t\t\t\tEpoch: 60/200\tTrain loss: 1.29\tValid loss: 21.06\n",
      "\t\t\t\tEpoch: 61/200\tTrain loss: 1.23\tValid loss: 21.07\n",
      "\t\t\t\tEpoch: 62/200\tTrain loss: 1.15\tValid loss: 21.07\n",
      "\t\t\t\tEpoch: 63/200\tTrain loss: 1.19\tValid loss: 21.05\n",
      "\t\t\t\tEpoch: 64/200\tTrain loss: 1.23\tValid loss: 20.96\n",
      "\t\t\t\tEpoch: 65/200\tTrain loss: 1.18\tValid loss: 21.00\n",
      "\t\t\t\tEpoch: 66/200\tTrain loss: 1.18\tValid loss: 20.93\n",
      "\t\t\t\tEpoch: 67/200\tTrain loss: 1.17\tValid loss: 20.92\n",
      "\t\t\t\tEpoch: 68/200\tTrain loss: 1.15\tValid loss: 20.93\n",
      "\t\t\t\tEpoch: 69/200\tTrain loss: 1.07\tValid loss: 20.92\n",
      "\t\t\t\tEpoch: 70/200\tTrain loss: 1.08\tValid loss: 20.89\n",
      "\t\t\t\tEpoch: 71/200\tTrain loss: 1.02\tValid loss: 20.90\n",
      "\t\t\t\tEpoch: 72/200\tTrain loss: 1.01\tValid loss: 20.80\n",
      "\t\t\t\tEpoch: 73/200\tTrain loss: 1.02\tValid loss: 20.80\n",
      "\t\t\t\tEpoch: 74/200\tTrain loss: 0.97\tValid loss: 20.78\n",
      "\t\t\t\tEpoch: 75/200\tTrain loss: 0.99\tValid loss: 20.79\n",
      "\t\t\t\tEpoch: 76/200\tTrain loss: 1.01\tValid loss: 20.82\n",
      "\t\t\t\tEpoch: 77/200\tTrain loss: 0.97\tValid loss: 20.75\n",
      "\t\t\t\tEpoch: 78/200\tTrain loss: 0.97\tValid loss: 20.73\n",
      "\t\t\t\tEpoch: 79/200\tTrain loss: 0.96\tValid loss: 20.71\n",
      "\t\t\t\tEpoch: 80/200\tTrain loss: 0.88\tValid loss: 20.73\n",
      "\t\t\t\tEpoch: 81/200\tTrain loss: 0.87\tValid loss: 20.72\n",
      "\t\t\t\tEpoch: 82/200\tTrain loss: 0.88\tValid loss: 20.74\n",
      "\t\t\t\tEpoch: 83/200\tTrain loss: 0.89\tValid loss: 20.69\n",
      "\t\t\t\tEpoch: 84/200\tTrain loss: 0.84\tValid loss: 20.68\n",
      "\t\t\t\tEpoch: 85/200\tTrain loss: 0.87\tValid loss: 20.73\n",
      "\t\t\t\tEpoch: 86/200\tTrain loss: 0.85\tValid loss: 20.74\n",
      "\t\t\t\tEpoch: 87/200\tTrain loss: 0.86\tValid loss: 20.73\n",
      "\t\t\t\tEpoch: 88/200\tTrain loss: 0.80\tValid loss: 20.66\n",
      "\t\t\t\tEpoch: 89/200\tTrain loss: 0.79\tValid loss: 20.65\n",
      "\t\t\t\tEpoch: 90/200\tTrain loss: 0.80\tValid loss: 20.62\n",
      "\t\t\t\tEpoch: 91/200\tTrain loss: 0.78\tValid loss: 20.62\n",
      "\t\t\t\tEpoch: 92/200\tTrain loss: 0.80\tValid loss: 20.67\n",
      "\t\t\t\tEpoch: 93/200\tTrain loss: 0.79\tValid loss: 20.59\n",
      "\t\t\t\tEpoch: 94/200\tTrain loss: 0.79\tValid loss: 20.66\n",
      "\t\t\t\tEpoch: 95/200\tTrain loss: 0.75\tValid loss: 20.59\n",
      "\t\t\t\tEpoch: 96/200\tTrain loss: 0.77\tValid loss: 20.56\n",
      "\t\t\t\tEpoch: 97/200\tTrain loss: 0.74\tValid loss: 20.55\n",
      "\t\t\t\tEpoch: 98/200\tTrain loss: 0.71\tValid loss: 20.54\n",
      "\t\t\t\tEpoch: 99/200\tTrain loss: 0.72\tValid loss: 20.51\n",
      "\t\t\t\tEpoch: 100/200\tTrain loss: 0.70\tValid loss: 20.51\n",
      "\t\t\t\tEpoch: 101/200\tTrain loss: 0.73\tValid loss: 20.55\n",
      "\t\t\t\tEpoch: 102/200\tTrain loss: 0.69\tValid loss: 20.54\n",
      "\t\t\t\tEpoch: 103/200\tTrain loss: 0.72\tValid loss: 20.55\n",
      "\t\t\t\tEpoch: 104/200\tTrain loss: 0.68\tValid loss: 20.53\n",
      "\t\t\t\tEpoch: 105/200\tTrain loss: 0.68\tValid loss: 20.51\n",
      "\t\t\t\tEpoch: 106/200\tTrain loss: 0.65\tValid loss: 20.52\n",
      "\t\t\t\tEpoch: 107/200\tTrain loss: 0.66\tValid loss: 20.48\n",
      "\t\t\t\tEpoch: 108/200\tTrain loss: 0.67\tValid loss: 20.50\n",
      "\t\t\t\tEpoch: 109/200\tTrain loss: 0.67\tValid loss: 20.47\n",
      "\t\t\t\tEpoch: 110/200\tTrain loss: 0.68\tValid loss: 20.53\n",
      "\t\t\t\tEpoch: 111/200\tTrain loss: 0.65\tValid loss: 20.56\n",
      "\t\t\t\tEpoch: 112/200\tTrain loss: 0.65\tValid loss: 20.49\n",
      "\t\t\t\tEpoch: 113/200\tTrain loss: 0.67\tValid loss: 20.51\n",
      "\t\t\t\tEpoch: 114/200\tTrain loss: 0.63\tValid loss: 20.43\n",
      "\t\t\t\tEpoch: 115/200\tTrain loss: 0.62\tValid loss: 20.42\n",
      "\t\t\t\tEpoch: 116/200\tTrain loss: 0.63\tValid loss: 20.45\n",
      "\t\t\t\tEpoch: 117/200\tTrain loss: 0.59\tValid loss: 20.39\n",
      "\t\t\t\tEpoch: 118/200\tTrain loss: 0.56\tValid loss: 20.39\n",
      "\t\t\t\tEpoch: 119/200\tTrain loss: 0.63\tValid loss: 20.39\n",
      "\t\t\t\tEpoch: 120/200\tTrain loss: 0.60\tValid loss: 20.37\n",
      "\t\t\t\tEpoch: 121/200\tTrain loss: 0.57\tValid loss: 20.42\n",
      "\t\t\t\tEpoch: 122/200\tTrain loss: 0.59\tValid loss: 20.40\n",
      "\t\t\t\tEpoch: 123/200\tTrain loss: 0.59\tValid loss: 20.43\n",
      "\t\t\t\tEpoch: 124/200\tTrain loss: 0.54\tValid loss: 20.41\n",
      "\t\t\t\tEpoch: 125/200\tTrain loss: 0.55\tValid loss: 20.37\n",
      "\t\t\t\tEpoch: 126/200\tTrain loss: 0.54\tValid loss: 20.41\n",
      "\t\t\t\tEpoch: 127/200\tTrain loss: 0.54\tValid loss: 20.42\n",
      "\t\t\t\tEpoch: 128/200\tTrain loss: 0.55\tValid loss: 20.36\n",
      "\t\t\t\tEpoch: 129/200\tTrain loss: 0.53\tValid loss: 20.39\n",
      "\t\t\t\tEpoch: 130/200\tTrain loss: 0.55\tValid loss: 20.38\n",
      "\t\t\t\tEpoch: 131/200\tTrain loss: 0.52\tValid loss: 20.35\n",
      "\t\t\t\tEpoch: 132/200\tTrain loss: 0.52\tValid loss: 20.35\n",
      "\t\t\t\tEpoch: 133/200\tTrain loss: 0.53\tValid loss: 20.32\n",
      "\t\t\t\tEpoch: 134/200\tTrain loss: 0.52\tValid loss: 20.34\n",
      "\t\t\t\tEpoch: 135/200\tTrain loss: 0.54\tValid loss: 20.36\n",
      "\t\t\t\tEpoch: 136/200\tTrain loss: 0.50\tValid loss: 20.35\n",
      "\t\t\t\tEpoch: 137/200\tTrain loss: 0.49\tValid loss: 20.35\n",
      "\t\t\t\tEpoch: 138/200\tTrain loss: 0.54\tValid loss: 20.37\n",
      "\t\t\t\tEpoch: 139/200\tTrain loss: 0.48\tValid loss: 20.35\n",
      "\t\t\t\tEpoch: 140/200\tTrain loss: 0.49\tValid loss: 20.32\n",
      "\t\t\t\tEpoch: 141/200\tTrain loss: 0.48\tValid loss: 20.32\n",
      "\t\t\t\tEpoch: 142/200\tTrain loss: 0.48\tValid loss: 20.32\n",
      "\t\t\t\tEpoch: 143/200\tTrain loss: 0.48\tValid loss: 20.34\n",
      "\t\t\t\tEpoch: 144/200\tTrain loss: 0.46\tValid loss: 20.36\n",
      "\t\t\t\tEpoch: 145/200\tTrain loss: 0.47\tValid loss: 20.35\n",
      "\t\t\t\tEpoch: 146/200\tTrain loss: 0.46\tValid loss: 20.30\n",
      "\t\t\t\tEpoch: 147/200\tTrain loss: 0.47\tValid loss: 20.35\n",
      "\t\t\t\tEpoch: 148/200\tTrain loss: 0.47\tValid loss: 20.32\n",
      "\t\t\t\tEpoch: 149/200\tTrain loss: 0.46\tValid loss: 20.33\n",
      "\t\t\t\tEpoch: 150/200\tTrain loss: 0.45\tValid loss: 20.36\n",
      "\t\t\t\tEpoch: 151/200\tTrain loss: 0.46\tValid loss: 20.32\n",
      "\t\t\t\tEpoch: 152/200\tTrain loss: 0.46\tValid loss: 20.31\n",
      "\t\t\t\tEpoch: 153/200\tTrain loss: 0.47\tValid loss: 20.31\n",
      "\t\t\t\tEpoch: 154/200\tTrain loss: 0.43\tValid loss: 20.38\n",
      "\t\t\t\tEpoch: 155/200\tTrain loss: 0.45\tValid loss: 20.32\n",
      "\t\t\t\tEpoch: 156/200\tTrain loss: 0.48\tValid loss: 20.34\n",
      "\t\t\t\tEpoch: 157/200\tTrain loss: 0.47\tValid loss: 20.25\n",
      "\t\t\t\tEpoch: 158/200\tTrain loss: 0.45\tValid loss: 20.30\n",
      "\t\t\t\tEpoch: 159/200\tTrain loss: 0.46\tValid loss: 20.32\n",
      "\t\t\t\tEpoch: 160/200\tTrain loss: 0.43\tValid loss: 20.34\n",
      "\t\t\t\tEpoch: 161/200\tTrain loss: 0.44\tValid loss: 20.28\n",
      "\t\t\t\tEpoch: 162/200\tTrain loss: 0.41\tValid loss: 20.28\n",
      "\t\t\t\tEpoch: 163/200\tTrain loss: 0.42\tValid loss: 20.24\n",
      "\t\t\t\tEpoch: 164/200\tTrain loss: 0.44\tValid loss: 20.25\n",
      "\t\t\t\tEpoch: 165/200\tTrain loss: 0.41\tValid loss: 20.28\n",
      "\t\t\t\tEpoch: 166/200\tTrain loss: 0.40\tValid loss: 20.24\n",
      "\t\t\t\tEpoch: 167/200\tTrain loss: 0.39\tValid loss: 20.25\n",
      "\t\t\t\tEpoch: 168/200\tTrain loss: 0.42\tValid loss: 20.29\n",
      "\t\t\t\tEpoch: 169/200\tTrain loss: 0.40\tValid loss: 20.26\n",
      "\t\t\t\tEpoch: 170/200\tTrain loss: 0.43\tValid loss: 20.28\n",
      "\t\t\t\tEpoch: 171/200\tTrain loss: 0.39\tValid loss: 20.29\n",
      "\t\t\t\tEpoch: 172/200\tTrain loss: 0.40\tValid loss: 20.30\n",
      "\t\t\t\tEpoch: 173/200\tTrain loss: 0.40\tValid loss: 20.26\n",
      "\t\t\t\tEpoch: 174/200\tTrain loss: 0.40\tValid loss: 20.23\n",
      "\t\t\t\tEpoch: 175/200\tTrain loss: 0.38\tValid loss: 20.27\n",
      "\t\t\t\tEpoch: 176/200\tTrain loss: 0.40\tValid loss: 20.23\n",
      "\t\t\t\tEpoch: 177/200\tTrain loss: 0.40\tValid loss: 20.24\n",
      "\t\t\t\tEpoch: 178/200\tTrain loss: 0.38\tValid loss: 20.26\n",
      "\t\t\t\tEpoch: 179/200\tTrain loss: 0.39\tValid loss: 20.28\n",
      "\t\t\t\tEpoch: 180/200\tTrain loss: 0.40\tValid loss: 20.26\n",
      "\t\t\t\tEpoch: 181/200\tTrain loss: 0.38\tValid loss: 20.23\n",
      "\t\t\t\tEpoch: 182/200\tTrain loss: 0.39\tValid loss: 20.22\n",
      "\t\t\t\tEpoch: 183/200\tTrain loss: 0.38\tValid loss: 20.24\n",
      "\t\t\t\tEpoch: 184/200\tTrain loss: 0.37\tValid loss: 20.20\n",
      "\t\t\t\tEpoch: 185/200\tTrain loss: 0.36\tValid loss: 20.23\n",
      "\t\t\t\tEpoch: 186/200\tTrain loss: 0.37\tValid loss: 20.15\n",
      "\t\t\t\tEpoch: 187/200\tTrain loss: 0.38\tValid loss: 20.22\n",
      "\t\t\t\tEpoch: 188/200\tTrain loss: 0.36\tValid loss: 20.25\n",
      "\t\t\t\tEpoch: 189/200\tTrain loss: 0.38\tValid loss: 20.23\n",
      "\t\t\t\tEpoch: 190/200\tTrain loss: 0.36\tValid loss: 20.27\n",
      "\t\t\t\tEpoch: 191/200\tTrain loss: 0.36\tValid loss: 20.26\n",
      "\t\t\t\tEpoch: 192/200\tTrain loss: 0.38\tValid loss: 20.23\n",
      "\t\t\t\tEpoch: 193/200\tTrain loss: 0.35\tValid loss: 20.25\n",
      "\t\t\t\tEpoch: 194/200\tTrain loss: 0.36\tValid loss: 20.20\n",
      "\t\t\t\tEpoch: 195/200\tTrain loss: 0.36\tValid loss: 20.22\n",
      "\t\t\t\tEpoch: 196/200\tTrain loss: 0.35\tValid loss: 20.19\n",
      "\t\t\t\tEpoch: 197/200\tTrain loss: 0.34\tValid loss: 20.23\n",
      "\t\t\t\tEpoch: 198/200\tTrain loss: 0.35\tValid loss: 20.26\n",
      "\t\t\t\tEpoch: 199/200\tTrain loss: 0.35\tValid loss: 20.24\n",
      "\t\t\t\tEpoch: 200/200\tTrain loss: 0.35\tValid loss: 20.20\n",
      "final train loss: 0.346\tfinal val loss: 20.203\n",
      "\n",
      "\n",
      "========================================\n",
      "learning rate 1.00e-03\n",
      "========================================\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t\t\t\tEpoch: 1/200\tTrain loss: 74.78\tValid loss: 74.03\n",
      "\t\t\t\tEpoch: 2/200\tTrain loss: 73.31\tValid loss: 72.73\n",
      "\t\t\t\tEpoch: 3/200\tTrain loss: 71.69\tValid loss: 71.55\n",
      "\t\t\t\tEpoch: 4/200\tTrain loss: 70.27\tValid loss: 70.40\n",
      "\t\t\t\tEpoch: 5/200\tTrain loss: 68.79\tValid loss: 69.27\n",
      "\t\t\t\tEpoch: 6/200\tTrain loss: 67.38\tValid loss: 68.16\n",
      "\t\t\t\tEpoch: 7/200\tTrain loss: 65.99\tValid loss: 67.10\n",
      "\t\t\t\tEpoch: 8/200\tTrain loss: 64.63\tValid loss: 66.06\n",
      "\t\t\t\tEpoch: 9/200\tTrain loss: 63.25\tValid loss: 65.02\n",
      "\t\t\t\tEpoch: 10/200\tTrain loss: 61.86\tValid loss: 63.98\n",
      "\t\t\t\tEpoch: 11/200\tTrain loss: 60.65\tValid loss: 62.96\n",
      "\t\t\t\tEpoch: 12/200\tTrain loss: 59.23\tValid loss: 62.06\n",
      "\t\t\t\tEpoch: 13/200\tTrain loss: 58.00\tValid loss: 61.12\n",
      "\t\t\t\tEpoch: 14/200\tTrain loss: 56.75\tValid loss: 60.19\n",
      "\t\t\t\tEpoch: 15/200\tTrain loss: 55.57\tValid loss: 59.26\n",
      "\t\t\t\tEpoch: 16/200\tTrain loss: 54.35\tValid loss: 58.38\n",
      "\t\t\t\tEpoch: 17/200\tTrain loss: 53.11\tValid loss: 57.45\n",
      "\t\t\t\tEpoch: 18/200\tTrain loss: 52.06\tValid loss: 56.63\n",
      "\t\t\t\tEpoch: 19/200\tTrain loss: 51.01\tValid loss: 55.84\n",
      "\t\t\t\tEpoch: 20/200\tTrain loss: 49.93\tValid loss: 55.06\n",
      "\t\t\t\tEpoch: 21/200\tTrain loss: 48.69\tValid loss: 54.26\n",
      "\t\t\t\tEpoch: 22/200\tTrain loss: 47.67\tValid loss: 53.46\n",
      "\t\t\t\tEpoch: 23/200\tTrain loss: 46.68\tValid loss: 52.72\n",
      "\t\t\t\tEpoch: 24/200\tTrain loss: 45.57\tValid loss: 51.97\n",
      "\t\t\t\tEpoch: 25/200\tTrain loss: 44.60\tValid loss: 51.27\n",
      "\t\t\t\tEpoch: 26/200\tTrain loss: 43.83\tValid loss: 50.58\n",
      "\t\t\t\tEpoch: 27/200\tTrain loss: 42.84\tValid loss: 49.85\n",
      "\t\t\t\tEpoch: 28/200\tTrain loss: 41.98\tValid loss: 49.23\n",
      "\t\t\t\tEpoch: 29/200\tTrain loss: 41.17\tValid loss: 48.63\n",
      "\t\t\t\tEpoch: 30/200\tTrain loss: 40.29\tValid loss: 48.02\n",
      "\t\t\t\tEpoch: 31/200\tTrain loss: 39.30\tValid loss: 47.50\n",
      "\t\t\t\tEpoch: 32/200\tTrain loss: 38.61\tValid loss: 46.91\n",
      "\t\t\t\tEpoch: 33/200\tTrain loss: 37.69\tValid loss: 46.30\n",
      "\t\t\t\tEpoch: 34/200\tTrain loss: 37.08\tValid loss: 45.78\n",
      "\t\t\t\tEpoch: 35/200\tTrain loss: 36.22\tValid loss: 45.27\n",
      "\t\t\t\tEpoch: 36/200\tTrain loss: 35.50\tValid loss: 44.74\n",
      "\t\t\t\tEpoch: 37/200\tTrain loss: 34.66\tValid loss: 44.18\n",
      "\t\t\t\tEpoch: 38/200\tTrain loss: 34.14\tValid loss: 43.79\n",
      "\t\t\t\tEpoch: 39/200\tTrain loss: 33.63\tValid loss: 43.23\n",
      "\t\t\t\tEpoch: 40/200\tTrain loss: 32.90\tValid loss: 42.79\n",
      "\t\t\t\tEpoch: 41/200\tTrain loss: 32.13\tValid loss: 42.32\n",
      "\t\t\t\tEpoch: 42/200\tTrain loss: 31.50\tValid loss: 41.93\n",
      "\t\t\t\tEpoch: 43/200\tTrain loss: 30.93\tValid loss: 41.53\n",
      "\t\t\t\tEpoch: 44/200\tTrain loss: 30.31\tValid loss: 41.10\n",
      "\t\t\t\tEpoch: 45/200\tTrain loss: 29.85\tValid loss: 40.62\n",
      "\t\t\t\tEpoch: 46/200\tTrain loss: 29.03\tValid loss: 40.31\n",
      "\t\t\t\tEpoch: 47/200\tTrain loss: 28.84\tValid loss: 39.90\n",
      "\t\t\t\tEpoch: 48/200\tTrain loss: 28.07\tValid loss: 39.67\n",
      "\t\t\t\tEpoch: 49/200\tTrain loss: 27.82\tValid loss: 39.29\n",
      "\t\t\t\tEpoch: 50/200\tTrain loss: 27.19\tValid loss: 38.88\n",
      "\t\t\t\tEpoch: 51/200\tTrain loss: 26.53\tValid loss: 38.52\n",
      "\t\t\t\tEpoch: 52/200\tTrain loss: 26.32\tValid loss: 38.19\n",
      "\t\t\t\tEpoch: 53/200\tTrain loss: 25.81\tValid loss: 37.81\n",
      "\t\t\t\tEpoch: 54/200\tTrain loss: 25.30\tValid loss: 37.67\n",
      "\t\t\t\tEpoch: 55/200\tTrain loss: 24.66\tValid loss: 37.29\n",
      "\t\t\t\tEpoch: 56/200\tTrain loss: 24.38\tValid loss: 37.01\n",
      "\t\t\t\tEpoch: 57/200\tTrain loss: 24.22\tValid loss: 36.82\n",
      "\t\t\t\tEpoch: 58/200\tTrain loss: 23.52\tValid loss: 36.49\n",
      "\t\t\t\tEpoch: 59/200\tTrain loss: 23.24\tValid loss: 36.24\n",
      "\t\t\t\tEpoch: 60/200\tTrain loss: 22.77\tValid loss: 35.98\n",
      "\t\t\t\tEpoch: 61/200\tTrain loss: 22.37\tValid loss: 35.69\n",
      "\t\t\t\tEpoch: 62/200\tTrain loss: 22.17\tValid loss: 35.49\n",
      "\t\t\t\tEpoch: 63/200\tTrain loss: 21.76\tValid loss: 35.28\n",
      "\t\t\t\tEpoch: 64/200\tTrain loss: 21.36\tValid loss: 34.99\n",
      "\t\t\t\tEpoch: 65/200\tTrain loss: 20.97\tValid loss: 34.75\n",
      "\t\t\t\tEpoch: 66/200\tTrain loss: 20.72\tValid loss: 34.48\n",
      "\t\t\t\tEpoch: 67/200\tTrain loss: 20.49\tValid loss: 34.38\n",
      "\t\t\t\tEpoch: 68/200\tTrain loss: 20.07\tValid loss: 34.09\n",
      "\t\t\t\tEpoch: 69/200\tTrain loss: 19.94\tValid loss: 34.00\n",
      "\t\t\t\tEpoch: 70/200\tTrain loss: 19.59\tValid loss: 33.66\n",
      "\t\t\t\tEpoch: 71/200\tTrain loss: 19.07\tValid loss: 33.53\n",
      "\t\t\t\tEpoch: 72/200\tTrain loss: 18.95\tValid loss: 33.35\n",
      "\t\t\t\tEpoch: 73/200\tTrain loss: 18.43\tValid loss: 33.08\n",
      "\t\t\t\tEpoch: 74/200\tTrain loss: 18.46\tValid loss: 32.93\n",
      "\t\t\t\tEpoch: 75/200\tTrain loss: 18.26\tValid loss: 32.67\n",
      "\t\t\t\tEpoch: 76/200\tTrain loss: 17.93\tValid loss: 32.59\n",
      "\t\t\t\tEpoch: 77/200\tTrain loss: 17.62\tValid loss: 32.40\n",
      "\t\t\t\tEpoch: 78/200\tTrain loss: 17.45\tValid loss: 32.20\n",
      "\t\t\t\tEpoch: 79/200\tTrain loss: 16.96\tValid loss: 32.11\n",
      "\t\t\t\tEpoch: 80/200\tTrain loss: 16.77\tValid loss: 31.95\n",
      "\t\t\t\tEpoch: 81/200\tTrain loss: 16.73\tValid loss: 31.80\n",
      "\t\t\t\tEpoch: 82/200\tTrain loss: 16.39\tValid loss: 31.59\n",
      "\t\t\t\tEpoch: 83/200\tTrain loss: 16.18\tValid loss: 31.52\n",
      "\t\t\t\tEpoch: 84/200\tTrain loss: 15.98\tValid loss: 31.39\n",
      "\t\t\t\tEpoch: 85/200\tTrain loss: 15.85\tValid loss: 31.13\n",
      "\t\t\t\tEpoch: 86/200\tTrain loss: 15.58\tValid loss: 31.06\n",
      "\t\t\t\tEpoch: 87/200\tTrain loss: 15.27\tValid loss: 30.91\n",
      "\t\t\t\tEpoch: 88/200\tTrain loss: 14.91\tValid loss: 30.77\n",
      "\t\t\t\tEpoch: 89/200\tTrain loss: 15.02\tValid loss: 30.63\n",
      "\t\t\t\tEpoch: 90/200\tTrain loss: 14.58\tValid loss: 30.47\n",
      "\t\t\t\tEpoch: 91/200\tTrain loss: 14.65\tValid loss: 30.37\n",
      "\t\t\t\tEpoch: 92/200\tTrain loss: 14.35\tValid loss: 30.30\n",
      "\t\t\t\tEpoch: 93/200\tTrain loss: 14.34\tValid loss: 30.14\n",
      "\t\t\t\tEpoch: 94/200\tTrain loss: 13.96\tValid loss: 30.01\n",
      "\t\t\t\tEpoch: 95/200\tTrain loss: 14.00\tValid loss: 29.93\n",
      "\t\t\t\tEpoch: 96/200\tTrain loss: 13.56\tValid loss: 29.75\n",
      "\t\t\t\tEpoch: 97/200\tTrain loss: 13.56\tValid loss: 29.72\n",
      "\t\t\t\tEpoch: 98/200\tTrain loss: 13.55\tValid loss: 29.58\n",
      "\t\t\t\tEpoch: 99/200\tTrain loss: 13.19\tValid loss: 29.45\n",
      "\t\t\t\tEpoch: 100/200\tTrain loss: 13.15\tValid loss: 29.40\n",
      "\t\t\t\tEpoch: 101/200\tTrain loss: 12.96\tValid loss: 29.28\n",
      "\t\t\t\tEpoch: 102/200\tTrain loss: 12.73\tValid loss: 29.21\n",
      "\t\t\t\tEpoch: 103/200\tTrain loss: 12.48\tValid loss: 29.04\n",
      "\t\t\t\tEpoch: 104/200\tTrain loss: 12.60\tValid loss: 28.98\n",
      "\t\t\t\tEpoch: 105/200\tTrain loss: 12.45\tValid loss: 28.89\n",
      "\t\t\t\tEpoch: 106/200\tTrain loss: 12.12\tValid loss: 28.78\n",
      "\t\t\t\tEpoch: 107/200\tTrain loss: 12.12\tValid loss: 28.70\n",
      "\t\t\t\tEpoch: 108/200\tTrain loss: 11.94\tValid loss: 28.59\n",
      "\t\t\t\tEpoch: 109/200\tTrain loss: 11.87\tValid loss: 28.46\n",
      "\t\t\t\tEpoch: 110/200\tTrain loss: 11.53\tValid loss: 28.42\n",
      "\t\t\t\tEpoch: 111/200\tTrain loss: 11.52\tValid loss: 28.33\n",
      "\t\t\t\tEpoch: 112/200\tTrain loss: 11.49\tValid loss: 28.28\n",
      "\t\t\t\tEpoch: 113/200\tTrain loss: 11.51\tValid loss: 28.20\n",
      "\t\t\t\tEpoch: 114/200\tTrain loss: 11.08\tValid loss: 28.10\n",
      "\t\t\t\tEpoch: 115/200\tTrain loss: 11.03\tValid loss: 28.12\n",
      "\t\t\t\tEpoch: 116/200\tTrain loss: 10.84\tValid loss: 27.93\n",
      "\t\t\t\tEpoch: 117/200\tTrain loss: 10.91\tValid loss: 27.87\n",
      "\t\t\t\tEpoch: 118/200\tTrain loss: 10.54\tValid loss: 27.76\n",
      "\t\t\t\tEpoch: 119/200\tTrain loss: 10.52\tValid loss: 27.71\n",
      "\t\t\t\tEpoch: 120/200\tTrain loss: 10.49\tValid loss: 27.66\n",
      "\t\t\t\tEpoch: 121/200\tTrain loss: 10.37\tValid loss: 27.60\n",
      "\t\t\t\tEpoch: 122/200\tTrain loss: 10.14\tValid loss: 27.52\n",
      "\t\t\t\tEpoch: 123/200\tTrain loss: 10.28\tValid loss: 27.42\n",
      "\t\t\t\tEpoch: 124/200\tTrain loss: 10.27\tValid loss: 27.38\n",
      "\t\t\t\tEpoch: 125/200\tTrain loss: 10.03\tValid loss: 27.32\n",
      "\t\t\t\tEpoch: 126/200\tTrain loss: 9.81\tValid loss: 27.25\n",
      "\t\t\t\tEpoch: 127/200\tTrain loss: 9.88\tValid loss: 27.18\n",
      "\t\t\t\tEpoch: 128/200\tTrain loss: 9.61\tValid loss: 27.10\n",
      "\t\t\t\tEpoch: 129/200\tTrain loss: 9.61\tValid loss: 27.11\n",
      "\t\t\t\tEpoch: 130/200\tTrain loss: 9.52\tValid loss: 26.94\n",
      "\t\t\t\tEpoch: 131/200\tTrain loss: 9.42\tValid loss: 26.90\n",
      "\t\t\t\tEpoch: 132/200\tTrain loss: 9.33\tValid loss: 26.85\n",
      "\t\t\t\tEpoch: 133/200\tTrain loss: 9.22\tValid loss: 26.89\n",
      "\t\t\t\tEpoch: 134/200\tTrain loss: 9.15\tValid loss: 26.79\n",
      "\t\t\t\tEpoch: 135/200\tTrain loss: 9.10\tValid loss: 26.73\n",
      "\t\t\t\tEpoch: 136/200\tTrain loss: 9.02\tValid loss: 26.66\n",
      "\t\t\t\tEpoch: 137/200\tTrain loss: 9.00\tValid loss: 26.65\n",
      "\t\t\t\tEpoch: 138/200\tTrain loss: 8.86\tValid loss: 26.52\n",
      "\t\t\t\tEpoch: 139/200\tTrain loss: 8.60\tValid loss: 26.43\n",
      "\t\t\t\tEpoch: 140/200\tTrain loss: 8.64\tValid loss: 26.40\n",
      "\t\t\t\tEpoch: 141/200\tTrain loss: 8.49\tValid loss: 26.40\n",
      "\t\t\t\tEpoch: 142/200\tTrain loss: 8.48\tValid loss: 26.35\n",
      "\t\t\t\tEpoch: 143/200\tTrain loss: 8.42\tValid loss: 26.26\n",
      "\t\t\t\tEpoch: 144/200\tTrain loss: 8.19\tValid loss: 26.17\n",
      "\t\t\t\tEpoch: 145/200\tTrain loss: 8.18\tValid loss: 26.17\n",
      "\t\t\t\tEpoch: 146/200\tTrain loss: 8.26\tValid loss: 26.09\n",
      "\t\t\t\tEpoch: 147/200\tTrain loss: 8.07\tValid loss: 26.09\n",
      "\t\t\t\tEpoch: 148/200\tTrain loss: 8.01\tValid loss: 25.99\n",
      "\t\t\t\tEpoch: 149/200\tTrain loss: 7.91\tValid loss: 26.02\n",
      "\t\t\t\tEpoch: 150/200\tTrain loss: 7.78\tValid loss: 25.98\n",
      "\t\t\t\tEpoch: 151/200\tTrain loss: 7.77\tValid loss: 25.87\n",
      "\t\t\t\tEpoch: 152/200\tTrain loss: 7.69\tValid loss: 25.86\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t\t\t\tEpoch: 153/200\tTrain loss: 7.77\tValid loss: 25.78\n",
      "\t\t\t\tEpoch: 154/200\tTrain loss: 7.53\tValid loss: 25.74\n",
      "\t\t\t\tEpoch: 155/200\tTrain loss: 7.57\tValid loss: 25.73\n",
      "\t\t\t\tEpoch: 156/200\tTrain loss: 7.62\tValid loss: 25.68\n",
      "\t\t\t\tEpoch: 157/200\tTrain loss: 7.53\tValid loss: 25.66\n",
      "\t\t\t\tEpoch: 158/200\tTrain loss: 7.42\tValid loss: 25.64\n",
      "\t\t\t\tEpoch: 159/200\tTrain loss: 7.45\tValid loss: 25.59\n",
      "\t\t\t\tEpoch: 160/200\tTrain loss: 7.29\tValid loss: 25.48\n",
      "\t\t\t\tEpoch: 161/200\tTrain loss: 7.11\tValid loss: 25.45\n",
      "\t\t\t\tEpoch: 162/200\tTrain loss: 7.08\tValid loss: 25.48\n",
      "\t\t\t\tEpoch: 163/200\tTrain loss: 6.98\tValid loss: 25.50\n",
      "\t\t\t\tEpoch: 164/200\tTrain loss: 7.08\tValid loss: 25.31\n",
      "\t\t\t\tEpoch: 165/200\tTrain loss: 7.11\tValid loss: 25.32\n",
      "\t\t\t\tEpoch: 166/200\tTrain loss: 6.83\tValid loss: 25.29\n",
      "\t\t\t\tEpoch: 167/200\tTrain loss: 6.93\tValid loss: 25.29\n",
      "\t\t\t\tEpoch: 168/200\tTrain loss: 6.79\tValid loss: 25.23\n",
      "\t\t\t\tEpoch: 169/200\tTrain loss: 6.76\tValid loss: 25.18\n",
      "\t\t\t\tEpoch: 170/200\tTrain loss: 6.65\tValid loss: 25.15\n",
      "\t\t\t\tEpoch: 171/200\tTrain loss: 6.54\tValid loss: 25.07\n",
      "\t\t\t\tEpoch: 172/200\tTrain loss: 6.55\tValid loss: 25.08\n",
      "\t\t\t\tEpoch: 173/200\tTrain loss: 6.54\tValid loss: 25.03\n",
      "\t\t\t\tEpoch: 174/200\tTrain loss: 6.53\tValid loss: 25.04\n",
      "\t\t\t\tEpoch: 175/200\tTrain loss: 6.44\tValid loss: 24.98\n",
      "\t\t\t\tEpoch: 176/200\tTrain loss: 6.50\tValid loss: 24.93\n",
      "\t\t\t\tEpoch: 177/200\tTrain loss: 6.33\tValid loss: 24.87\n",
      "\t\t\t\tEpoch: 178/200\tTrain loss: 6.24\tValid loss: 24.89\n",
      "\t\t\t\tEpoch: 179/200\tTrain loss: 6.23\tValid loss: 24.90\n",
      "\t\t\t\tEpoch: 180/200\tTrain loss: 6.24\tValid loss: 24.83\n",
      "\t\t\t\tEpoch: 181/200\tTrain loss: 6.14\tValid loss: 24.80\n",
      "\t\t\t\tEpoch: 182/200\tTrain loss: 6.18\tValid loss: 24.77\n",
      "\t\t\t\tEpoch: 183/200\tTrain loss: 5.98\tValid loss: 24.71\n",
      "\t\t\t\tEpoch: 184/200\tTrain loss: 5.86\tValid loss: 24.69\n",
      "\t\t\t\tEpoch: 185/200\tTrain loss: 5.92\tValid loss: 24.62\n",
      "\t\t\t\tEpoch: 186/200\tTrain loss: 5.76\tValid loss: 24.63\n",
      "\t\t\t\tEpoch: 187/200\tTrain loss: 5.92\tValid loss: 24.63\n",
      "\t\t\t\tEpoch: 188/200\tTrain loss: 5.91\tValid loss: 24.60\n",
      "\t\t\t\tEpoch: 189/200\tTrain loss: 5.86\tValid loss: 24.55\n",
      "\t\t\t\tEpoch: 190/200\tTrain loss: 5.83\tValid loss: 24.56\n",
      "\t\t\t\tEpoch: 191/200\tTrain loss: 5.69\tValid loss: 24.43\n",
      "\t\t\t\tEpoch: 192/200\tTrain loss: 5.72\tValid loss: 24.49\n",
      "\t\t\t\tEpoch: 193/200\tTrain loss: 5.65\tValid loss: 24.45\n",
      "\t\t\t\tEpoch: 194/200\tTrain loss: 5.53\tValid loss: 24.44\n",
      "\t\t\t\tEpoch: 195/200\tTrain loss: 5.48\tValid loss: 24.41\n",
      "\t\t\t\tEpoch: 196/200\tTrain loss: 5.58\tValid loss: 24.37\n",
      "\t\t\t\tEpoch: 197/200\tTrain loss: 5.45\tValid loss: 24.31\n",
      "\t\t\t\tEpoch: 198/200\tTrain loss: 5.42\tValid loss: 24.30\n",
      "\t\t\t\tEpoch: 199/200\tTrain loss: 5.32\tValid loss: 24.29\n",
      "\t\t\t\tEpoch: 200/200\tTrain loss: 5.42\tValid loss: 24.25\n",
      "final train loss: 5.420\tfinal val loss: 24.250\n"
     ]
    }
   ],
   "source": [
    "epochs = 200\n",
    "\n",
    "for resnet, learning_rate in zip(nets, lrs):\n",
    "    \n",
    "    print('\\n\\n' + '=' * 40 + '\\nlearning rate %.2e\\n' % learning_rate + '=' * 40)\n",
    "    \n",
    "    resnet.to(device)\n",
    "    optimizer = torch.optim.SGD(resnet.parameters(), lr = learning_rate, momentum = momentum)\n",
    "    \n",
    "    train_loss, val_loss = train(\n",
    "        model = resnet, tloader =  train_loader, vloader = val_loader, \n",
    "        lf = loss_fn, epochs = epochs, optim = optimizer)\n",
    "    \n",
    "    print('final train loss: %.3f\\tfinal val loss: %.3f' % (train_loss[-1], val_loss[-1]))\n",
    "    resnet.to('cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "31f80bde",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "========================================\n",
      "learning rate 1.00e+00\n",
      "========================================\n",
      "Training Accuracy:\t1.000\n",
      "Validation Accuracy:\t0.620\n",
      "\n",
      "\n",
      "========================================\n",
      "learning rate 1.00e-01\n",
      "========================================\n",
      "Training Accuracy:\t1.000\n",
      "Validation Accuracy:\t0.684\n",
      "\n",
      "\n",
      "========================================\n",
      "learning rate 1.00e-02\n",
      "========================================\n",
      "Training Accuracy:\t1.000\n",
      "Validation Accuracy:\t0.681\n",
      "\n",
      "\n",
      "========================================\n",
      "learning rate 1.00e-03\n",
      "========================================\n",
      "Training Accuracy:\t0.999\n",
      "Validation Accuracy:\t0.676\n"
     ]
    }
   ],
   "source": [
    "# Get accuracies\n",
    "\n",
    "for resnet, learning_rate in zip(nets, lrs):\n",
    "    print('\\n\\n' + '=' * 40 + '\\nlearning rate %.2e\\n' % learning_rate + '=' * 40)\n",
    "    print(\"Training Accuracy:\\t%.3f\" % accuracy(resnet, train_loader, dummifyer))\n",
    "    print(\"Validation Accuracy:\\t%.3f\" % accuracy(resnet, val_loader, dummifyer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0636a50f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
